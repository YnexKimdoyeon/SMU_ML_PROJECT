{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56def565",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f267fbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>번호</th>\n",
       "      <th>약품일반성분명코드(ATC코드)</th>\n",
       "      <th>질병명</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>['R03A']</td>\n",
       "      <td>천식</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['C07AG02']</td>\n",
       "      <td>고혈압</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>['J01D']</td>\n",
       "      <td>세균감염</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>['S', 'S']</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>['M01AE']</td>\n",
       "      <td>관절염</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   번호 약품일반성분명코드(ATC코드)   질병명\n",
       "0   1         ['R03A']    천식\n",
       "1   2      ['C07AG02']   고혈압\n",
       "2   3         ['J01D']  세균감염\n",
       "3   4       ['S', 'S']   non\n",
       "4   5        ['M01AE']   관절염"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/final_data.csv\", encoding='utf-8-sig')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08a6a3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 173055 entries, 0 to 199999\n",
      "Data columns (total 2 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   약품일반성분명코드(ATC코드)  173055 non-null  object\n",
      " 1   질병명               173055 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 4.0+ MB\n"
     ]
    }
   ],
   "source": [
    "#질병명이 non인 경우 -> 결측치이므로 제거\n",
    "data = data[data['질병명']!='non']\n",
    "\n",
    "#번호 열은 의미없는 값 -> 드랍\n",
    "data = data.drop(columns=['번호'])\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2a665f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1aca45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATC 코드 vocabulary 생성\n",
    "code_counter = Counter(code \n",
    "                       for _, row in data.iterrows()\n",
    "                       for code in eval(row['약품일반성분명코드(ATC코드)']))\n",
    "\n",
    "voca = {code: idx+1 for idx, code in enumerate(code_counter)} #vocabulary 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b785719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADHD' '고지혈증' '고혈압' '관절염' '기침' '녹내장' '당뇨병' '대사질환' '말초혈관질환' '바이러스감염'\n",
      " '배뇨장애' '변비' '복합감염' '불면증' '불안장애' '비염' '세균감염' '심부전' '안과감염' '안질환' '역류성식도염'\n",
      " '영양결핍' '우울증' '위염' '정신질환' '진균감염' '천식' '통증' '통풍' '피부감염' '피부염' '해독' '혈전예방'\n",
      " '호흡기감염' '호흡기질환']\n"
     ]
    }
   ],
   "source": [
    "# 라벨 인코딩\n",
    "label_encoder = LabelEncoder()\n",
    "data['질병명'] = label_encoder.fit_transform(data['질병명'])\n",
    "# print(label_encoder.classes_)\n",
    "print(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd393313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 파이토치 데이터셋 정의\n",
    "class PrescriptionDataset(Dataset):\n",
    "    def __init__(self, df, voca, label_encoder, max_len=5):\n",
    "        self.voca = voca\n",
    "        self.label_encoder = label_encoder\n",
    "        self.max_len = max_len\n",
    "        self.samples = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            token_ids = [voca.get(code, 0) for code in row['약품일반성분명코드(ATC코드)']]\n",
    "            token_ids = token_ids[:max_len] + [0] * (max_len - len(token_ids))\n",
    "            label_id = row['질병명']\n",
    "            self.samples.append((torch.tensor(token_ids), torch.tensor(label_id)))\n",
    "\n",
    "    def __len__(self): # len() 호출 시\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx): # 인덱스[] 호출 시\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d6aee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 트랜스포머 분류 모델 정의\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, nhead, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size + 1, embed_dim, padding_idx=0)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):  # x: [batch, seq_len]\n",
    "        x = self.embedding(x)           # [batch, seq_len, embed_dim]\n",
    "        x = x.permute(1, 0, 2)          # [seq_len, batch, embed_dim]\n",
    "        out = self.encoder(x)           # [seq_len, batch, embed_dim]\n",
    "        out = out.mean(dim=0)           # mean pooling\n",
    "        return self.fc(out)             # [batch, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16611dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PrescriptionDataset(train_df, voca, label_encoder)\n",
    "test_dataset = PrescriptionDataset(test_df, voca, label_encoder)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b0cfc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moonp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 157051.7238\n",
      "Epoch 2, Loss: 156476.3330\n",
      "Epoch 3, Loss: 156367.6575\n",
      "Epoch 4, Loss: 156319.2974\n",
      "Epoch 5, Loss: 156298.4047\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m output = model(x_batch)\n\u001b[32m     18\u001b[39m loss = criterion(output, y_batch)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m optimizer.step()\n\u001b[32m     21\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\moonp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\moonp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\moonp\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = TransformerClassifier(\n",
    "    vocab_size=len(voca),\n",
    "    embed_dim=32,\n",
    "    nhead=4,\n",
    "    num_layers=2,\n",
    "    num_classes=len(label_encoder.classes_)\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
